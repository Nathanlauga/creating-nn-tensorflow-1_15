{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a basic neural network with Tensorflow v1.15\n",
    "\n",
    "This notebook is inspired by some code available in the [Coursera Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning#courses) and has the goal to reproduce the same logic as [Tensorflow playground](https://playground.tensorflow.org/) but in Python.\n",
    "\n",
    "So in summary you'll be able to choose the following hyperparameters :\n",
    "- number of hidden layers\n",
    "- number of units in each layers\n",
    "- loss function\n",
    "- activation per layer\n",
    "- optimizer to use\n",
    "- learning rate\n",
    "- batch size\n",
    "\n",
    "## Data used\n",
    "\n",
    "This notebook's goal is to create a few neural networks with `tensorflow 1.15` and try to submit the result into Kaggle.\n",
    "\n",
    "[Competition link](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/).\n",
    "\n",
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check GPU is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 10038302858391953019),\n",
       " _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 9300626239435706983),\n",
       " _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 2898054394965409782),\n",
       " _DeviceAttributes(/job:localhost/replica:0/task:0/device:GPU:0, GPU, 1267990528, 17442708505485129454)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    devices = sess.list_devices()\n",
    "sess.close()\n",
    "\n",
    "devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available(\n",
    "    cuda_only=False, min_cuda_compute_capability=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 81)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'SalePrice'\n",
    "# cols_to_keep = ['LotArea', 'TotalBsmtSF'] + [target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select variable correlated with target `SalePrice` (threshold > 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to get weighted correlation \n",
    "# for categoricals variable\n",
    "\n",
    "def corr_num_cat_var(df, num_var, cat_var):\n",
    "    \"\"\"Compute weighted correlation for a \n",
    "    categorical variable.\n",
    "    \n",
    "    \"\"\"\n",
    "    dummies = pd.get_dummies(df[cat_var])\n",
    "    sum_df = dummies.sum()\n",
    "    \n",
    "    dummies[num_var] = df[num_var]\n",
    "    corr = dummies.corr()\n",
    "    corr = corr[num_var].iloc[:-1]\n",
    "    \n",
    "    weighted_corr = np.sum(corr*sum_df) / len(df)\n",
    "    \n",
    "    return weighted_corr\n",
    "\n",
    "def get_correlatd_cat_feats(df, target, threshold):\n",
    "    \"\"\"Returns feature names that\n",
    "    are correlated to the target\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame\n",
    "        Dataframe to work on\n",
    "    target: str\n",
    "        target name\n",
    "    threshold: float\n",
    "        correlation threshold to keep feature\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    list:\n",
    "        list of categorical features to keep\n",
    "    \"\"\"\n",
    "    cat_vars = train.select_dtypes('object').columns\n",
    "    corr_df = pd.Series()\n",
    "\n",
    "    for cat_var in cat_vars:\n",
    "        corr = corr_num_cat_var(train, target, cat_var)\n",
    "        corr_df.loc[cat_var] = corr\n",
    "\n",
    "    cat_feats = corr_df[corr_df.abs() > threshold].index.tolist()\n",
    "    return cat_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = train.corr()[target]\n",
    "\n",
    "threshold = 0.1\n",
    "num_feats = corr[corr.abs() > threshold].index.tolist()\n",
    "cat_feats = get_correlatd_cat_feats(train, target, threshold)\n",
    "\n",
    "cols_to_keep = num_feat + cat_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_na = train[cols_to_keep].isna().sum()\n",
    "df_na[df_na > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create preparation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "def prepare_data(df, cols_to_keep, target, \n",
    "                 dummies_cols=None, transformer=None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    df = df[cols_to_keep]\n",
    "    num_feats = df.select_dtypes('number').columns.tolist()\n",
    "    cat_feats = df.select_dtypes('object').columns.tolist()\n",
    "    \n",
    "    if target in num_feats:\n",
    "        num_feats = [f for f in num_feats if f != target]\n",
    "    \n",
    "    # Missing values\n",
    "    df[num_feats] = df[num_feats].fillna(0)\n",
    "    df[cat_feats] = df[cat_feats].fillna('Missing value')\n",
    "    \n",
    "    dummies_exists = dummies_cols is not None\n",
    "    if not dummies_exists:\n",
    "        dummies_cols = list()\n",
    "    else:\n",
    "        for c in dummies_cols:\n",
    "            df[c] = 0\n",
    "    \n",
    "    for feature in cat_feats:\n",
    "        dummies = pd.get_dummies(df[feature], prefix=feature)\n",
    "        if not dummies_exists:\n",
    "            df[dummies.columns] = dummies\n",
    "            dummies_cols += dummies.columns.tolist()\n",
    "        else:\n",
    "            for c in dummies.columns:\n",
    "                if c in dummies_cols:\n",
    "                    df[c] = dummies[c]\n",
    "                    \n",
    "        df = df.drop(columns=feature)\n",
    "    \n",
    "    if transformer is None:\n",
    "        transformer = Normalizer().fit(df[num_feats])\n",
    "\n",
    "    df[num_feats] = transformer.transform(df[num_feats])\n",
    "    \n",
    "    return df, transformer, dummies_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, transformer, dummies_cols = prepare_data(train, cols_to_keep, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(df):\n",
    "    X, y = df.drop(columns=target), df[target]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = get_train_test(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create NN\n",
    "\n",
    "### Initialize Weight and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_units, n_features, n_units_output=1):    \n",
    "    \"\"\"Initialize parameters Weight and bias and stored them\n",
    "    in a dictionnary following this format : {'Wl': tf.Variable, 'bl': tf.Variable, ...}.\n",
    "    \n",
    "    Where `l` is the layer number. \n",
    "    \n",
    "    The dictionnary also store number of layers L (output layer include)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_units: list\n",
    "        list of number of units per layer\n",
    "    n_features: int\n",
    "        number of features at input layer\n",
    "    n_units_output: int (default 1)\n",
    "        number of units at output layer\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict:\n",
    "        Dictionnary of parameters    \n",
    "    \"\"\"\n",
    "    tf.set_random_seed(1) \n",
    "    parameters = {}\n",
    "\n",
    "    n_units += [n_units_output]    \n",
    "    n_layers = len(n_units)\n",
    "    \n",
    "    parameters['L'] = n_layers\n",
    "    \n",
    "    for l in range(0,n_layers):\n",
    "        num_units = n_units[l]   \n",
    "        W = f'W{l}'\n",
    "        b = f'b{l}'\n",
    "\n",
    "        prev_n_units = n_features if l == 0 else n_units[l-1]\n",
    "\n",
    "        # Using Xavier initializer for Weight \n",
    "        # Todo custom\n",
    "        parameters[W] = tf.get_variable(name=W, shape=[prev_n_units, num_units], \n",
    "                                        initializer = tf.contrib.layers.xavier_initializer())\n",
    "        # Using zeros initializer for bias\n",
    "        parameters[b] = tf.get_variable(name=b, shape=[1,num_units], \n",
    "                                        initializer = tf.zeros_initializer())\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'L': 3,\n",
       " 'W0': <tf.Variable 'W0:0' shape=(20, 10) dtype=float32_ref>,\n",
       " 'b0': <tf.Variable 'b0:0' shape=(1, 10) dtype=float32_ref>,\n",
       " 'W1': <tf.Variable 'W1:0' shape=(10, 5) dtype=float32_ref>,\n",
       " 'b1': <tf.Variable 'b1:0' shape=(1, 5) dtype=float32_ref>,\n",
       " 'W2': <tf.Variable 'W2:0' shape=(5, 2) dtype=float32_ref>,\n",
       " 'b2': <tf.Variable 'b2:0' shape=(1, 2) dtype=float32_ref>}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test for a NN with 2 hidden layers, 20 features and 2 output units\n",
    "tf.reset_default_graph()   \n",
    "\n",
    "initialize_parameters(n_units=[10,5], n_features=20, n_units_output=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_layer(A, parameters, layer, activation='relu'):\n",
    "    \"\"\"Compute calculus for a given layer\n",
    "    \n",
    "    Follow this formula : Z_l = A_{l-1} * W_l + b_l\n",
    "    \n",
    "    if activation is set then this function returns : \n",
    "    A_l = activation_function(Z_l) \n",
    "    \n",
    "    Where :\n",
    "    - A is (1, nb units prev layer)\n",
    "    - W is (nb units prev layer, nb units current layer)\n",
    "    - b is (1, nb units current layer)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    A: tf.Variable\n",
    "        output of the previous layer\n",
    "    parameters: dict\n",
    "        Dictionnary of parameters  \n",
    "    layer: int\n",
    "        number of the current layer\n",
    "    activation: str (default relu)\n",
    "        activation function, if None then just \n",
    "        compute the linear calculus\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tf.Variable\n",
    "        Output of the layer calculus\n",
    "    \"\"\"\n",
    "    W = parameters['W'+str(layer)]\n",
    "    b = parameters['b'+str(layer)]\n",
    "    \n",
    "    Z = tf.add(tf.matmul(A,W), b, name=f'Z{layer}')\n",
    "    \n",
    "    if activation is not None:\n",
    "        return tf.nn.relu(Z, name=f'A{layer}') \n",
    "    \n",
    "    return Z\n",
    "\n",
    "def forward_propagation(X, parameters, activation='relu'):\n",
    "    \"\"\"Compute forward propagation given input X and\n",
    "    initialized parameters.\n",
    "    \n",
    "    You can custom activation functionfor the hidden layers\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: tf.Variable\n",
    "        input data shape (n_rows, n_features) \n",
    "    parameters: dict\n",
    "        Dictionnary of parameters  \n",
    "    activation: str (default relu)\n",
    "        activation function for hidden layers\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tf.Variable\n",
    "        Prediction output shape (n_rows, n_units_output)\n",
    "    \"\"\"\n",
    "    A = X\n",
    "    if 'L' not in parameters:\n",
    "        n_layers = int(len(parameters) / 2)\n",
    "    else:\n",
    "        n_layers = parameters['L']\n",
    "    \n",
    "    for l in range(0, n_layers):\n",
    "        act_l = activation if l != n_layers-1 else None\n",
    "        \n",
    "        A = compute_layer(A, parameters, layer=l, activation=act_l)\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Z2:0' shape=(?, 2) dtype=float32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test for a NN with 2 hidden layers, 20 features and 2 output units\n",
    "tf.reset_default_graph()   \n",
    "\n",
    "parameters = initialize_parameters(n_units=[10,5], n_features=20, n_units_output=2)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, 20), name='X')\n",
    "\n",
    "forward_propagation(X, parameters, activation='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute cost\n",
    "\n",
    "Handle the following cost function :\n",
    "\n",
    "-----\n",
    "\n",
    "- `MSE`: Mean square Error (or Quadratic Loss or L2 Loss)\n",
    "\n",
    "$MSE = \\frac{1}{n}\\sum_{i=1}^{n}{(y_i - \\hat{y_i})^2}$\n",
    "\n",
    "```python\n",
    "def loss(Y_pred, Y_real):\n",
    "    return tf.reduce_mean(tf.square(Y_pred - Y_real))\n",
    "```\n",
    "\n",
    "-----\n",
    "- `RMSE`: Root Mean square Error \n",
    "\n",
    "$RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}{(y_i - \\hat{y_i})^2}}$\n",
    "\n",
    "```python\n",
    "def loss(Y_pred, Y_real):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(Y_pred - Y_real)))\n",
    "```\n",
    "-----\n",
    "- `RMSE log`: Root Mean square Error (log values)\n",
    "\n",
    "$RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}{(\\log{(y_i)} - \\log{(\\hat{y_i})})^2}}$\n",
    "\n",
    "```python\n",
    "def loss(Y_pred, Y_real):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.abs(tf.log(Y_pred) - tf.log(Y_real))))\n",
    "```\n",
    "\n",
    "-----\n",
    "- `MAE`: Mean Absolute Error (or L1 Loss)\n",
    "\n",
    "$MAE = \\frac{1}{n} \\sum_{i=1}^{n}{|y_i - \\hat{y_i}|}$\n",
    "\n",
    "```python\n",
    "def loss(Y_pred, Y_real):\n",
    "    return tf.reduce_mean(tf.abs(Y_pred - Y_real))\n",
    "```\n",
    "\n",
    "-----\n",
    "- `Entropy`: Cross Entropy Loss (or Negative Log Likelihood)\n",
    "\n",
    "$\\text{Cross Entropy Loss} = - (y_i\\log{\\hat{y_i}} + (1-y_i)\\log{1-\\hat{y_i}})$\n",
    "\n",
    "```python\n",
    "def loss(Y_pred, Y_real):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels, logits))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(Y_pred, Y_real, loss='RMSE'):\n",
    "    \"\"\"Compute loss given a loss function\n",
    "    \n",
    "    Loss available : \n",
    "    - MSE\n",
    "    - RMSE\n",
    "    - RMSE log\n",
    "    - MAE\n",
    "    - Entropy\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Y_pred: tf.Variable\n",
    "        Predictions from the model\n",
    "    Y_real: tf.Variable\n",
    "        Real output\n",
    "    loss: str (default RMSE)\n",
    "        loss function to compute\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tf.Variable:\n",
    "        Loss of the model\n",
    "    \"\"\"\n",
    "    if loss == 'MSE':\n",
    "        return tf.reduce_mean(tf.square(Y_pred - Y_real), name='loss')\n",
    "    \n",
    "    elif loss == 'RMSE':\n",
    "        return tf.sqrt(tf.reduce_mean(tf.square(Y_pred - Y_real)), name='loss')\n",
    "    \n",
    "    elif loss == 'RMSE log':\n",
    "        return tf.sqrt(tf.reduce_mean(tf.square(tf.log(Y_pred) - tf.log(Y_real)), name='loss'))\n",
    "        \n",
    "    elif loss == 'MAE':\n",
    "        return tf.reduce_mean(tf.abs(Y_pred - Y_real), name='loss')\n",
    "    \n",
    "    elif loss == 'Entropy':\n",
    "        return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(Y_real, Y_pred), name='loss')\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "tf.reset_default_graph()   \n",
    "\n",
    "parameters = initialize_parameters(n_units=[10], n_features=5)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, 5), name='X')\n",
    "Y = tf.placeholder(tf.float32, shape=(None, 1), name='Y')\n",
    "\n",
    "Y_pred = forward_propagation(X, parameters, activation='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'loss:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss(Y_pred=Y_pred, Y_real=Y, loss='MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'loss_1:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss(Y_pred=Y_pred, Y_real=Y, loss='RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Sqrt:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss(Y_pred=Y_pred, Y_real=Y, loss='RMSE log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'loss_3:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss(Y_pred=Y_pred, Y_real=Y, loss='MAE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'loss_4:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss(Y_pred=Y_pred, Y_real=Y, loss='Entropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer selection\n",
    "\n",
    "This the list of possible optimizer to train the neural network :\n",
    "\n",
    "- GradientDescent: [`tf.train.GradientDescentOptimizer`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/train/GradientDescentOptimizer)\n",
    "- Momentum: [`tf.train.MomentumOptimizer`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/train/MomentumOptimizer)\n",
    "- Adagrad: [`tf.train.AdagradOptimizer`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/train/AdagradOptimizer)\n",
    "- Adadelta: [`tf.train.AdadeltaOptimizer`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/train/AdadeltaOptimizer)\n",
    "- RMSProp: [`tf.train.RMSPropOptimizer`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/train/RMSPropOptimizer)\n",
    "- Adam: [`tf.train.AdamOptimizer`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/train/AdamOptimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_optimizer(optimizer='GradientDescent'):\n",
    "    \"\"\"Select the optimizer\n",
    "    \n",
    "    Available optimizers:\n",
    "    - GradientDescent\n",
    "    - Momentum\n",
    "    - Adagrad\n",
    "    - Adadelta\n",
    "    - RMSProp\n",
    "    - Adam\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    optimizer: str (default GradientDescent)\n",
    "        optimizer to select\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tf.train.Optimizer\n",
    "        Optimizer object\n",
    "    \"\"\"\n",
    "    if optimizer == 'GradientDescent':\n",
    "        return tf.train.GradientDescentOptimizer\n",
    "    elif optimizer == 'Momentum':\n",
    "        return tf.train.MomentumOptimizer\n",
    "    elif optimizer == 'Adagrad':\n",
    "        return tf.train.AdagradOptimizer\n",
    "    elif optimizer == 'Adadelta':\n",
    "        return tf.train.AdadeltaOptimizer\n",
    "    elif optimizer == 'RMSProp':\n",
    "        return tf.train.RMSPropOptimizer\n",
    "    elif optimizer == 'Adam':\n",
    "        return tf.train.AdamOptimizer\n",
    "    \n",
    "    raise ValueError('Optimizer choosen not available please select a valid one')\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.training.adam.AdamOptimizer"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_optimizer(optimizer='Adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensorboard(sess, L, grads_and_vars, loss, name='dashboard'):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if not os.path.exists('summaries'):\n",
    "        os.mkdir('summaries')\n",
    "    if not os.path.exists(os.path.join('summaries',name)):\n",
    "        os.mkdir(os.path.join('summaries',name))\n",
    "\n",
    "    summ_writer = tf.summary.FileWriter(os.path.join('summaries',name), sess.graph)    \n",
    "    \n",
    "    # tf_loss_summary :     you feed in a value by means of a placeholder, \n",
    "    #                       whenever you need to publish this to the board\n",
    "    \n",
    "    with tf.name_scope('performance'):\n",
    "        tf_loss_ph = tf.placeholder(tf.float32,shape=None,name='loss_summary')\n",
    "        tf_loss_summary = tf.summary.scalar('loss', tf_loss_ph)\n",
    "\n",
    "        \n",
    "    # tf_gradnorm_summary : this calculates the l2 norm of the gradients \n",
    "    #                       of the last layer of your neural network. \n",
    "    #                       Gradient norm is a good indicator of whether the weights \n",
    "    #                       of the neural network are being properly updated. \n",
    "    #                       A too small gradient norm can indicate vanishing gradient \n",
    "    #                       or a too large gradient can imply exploding gradient phenomenon.\n",
    "    \n",
    "    for g, v in grads_and_vars:\n",
    "        if f'W{L-1}' in v.name:\n",
    "            with tf.name_scope('gradients'):\n",
    "                tf_last_grad_norm = tf.sqrt(tf.reduce_mean(g**2))\n",
    "                tf_gradnorm_summary = tf.summary.scalar('grad_norm', tf_last_grad_norm)\n",
    "                break\n",
    "                \n",
    "    performance_summaries = tf.summary.merge([tf_loss_summary])    \n",
    "    \n",
    "    return {\n",
    "        'writer': summ_writer, \n",
    "        'loss': tf_loss_ph, \n",
    "        'gradnorm': tf_gradnorm_summary,\n",
    "        'perf': performance_summaries\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate NN with optimizer for back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cost(train_costs, test_costs=None):\n",
    "    \"\"\"Plot the cost\n",
    "    \"\"\"\n",
    "    plt.plot(np.squeeze(train_costs), label='train')\n",
    "    if test_costs is not None:\n",
    "        plt.plot(np.squeeze(test_costs), label='test')\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per fives)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def fit(feed_dict_train, parameters, optimizer, loss, learning_rate, num_epochs,\n",
    "        epsilon, early_stopping, feed_dict_test, batch_size=None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # TODO : implement batch size\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    train_costs = list()\n",
    "    test_costs = list()\n",
    "    num_epochs = 100000000 if num_epochs is None else num_epochs\n",
    "    L = parameters['L']\n",
    "    old_param = list()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        \n",
    "        # board = create_tensorboard(sess, L, grads_and_vars, loss, name='dashboard')\n",
    "        # summ_writer = board['writer']\n",
    "        # tf_loss_ph = board['loss']\n",
    "        # tf_gradnorm_summary = board['gradnorm']\n",
    "        # performance_summaries = board['perf']\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            sess.run(optimizer, feed_dict=feed_dict_train)\n",
    "            \n",
    "            # epoch_l_train, gn_summ = sess.run([loss, tf_gradnorm_summary], feed_dict=feed_dict_train)\n",
    "            epoch_l_train = sess.run(loss, feed_dict=feed_dict_train)\n",
    "            epoch_l_test = sess.run(loss, feed_dict=feed_dict_test)\n",
    "            \n",
    "            # summ = sess.run(performance_summaries, feed_dict={tf_loss_ph: epoch_l_train})\n",
    "            \n",
    "            # summ_writer.add_summary(gn_summ, epoch)\n",
    "            # summ_writer.add_summary(summ, epoch)\n",
    "\n",
    "            if epoch % display_freq == 0:\n",
    "                lr = sess.run(learning_rate)\n",
    "                print (\"At epoch %i, learning rate is %.8f train cost is: %f and test cost is: %f\" % (\n",
    "                    epoch, lr, epoch_l_train, epoch_l_test))\n",
    "            \n",
    "            if str(epoch_l_train) == 'nan':\n",
    "                print(\"Cost is nan.\")\n",
    "                break\n",
    "            \n",
    "            train_costs.append(epoch_l_train)\n",
    "            test_costs.append(epoch_l_test)\n",
    "                \n",
    "            # Early stopping slow decrease\n",
    "            if len(train_costs) > 2:\n",
    "                if train_costs[-2] - train_costs[-1] < epsilon:\n",
    "                    print('Stop at epoch %i. because cost decrease is less than epsilon.'% (epoch))\n",
    "                    break\n",
    "                        \n",
    "            old_param.append(parameters)\n",
    "            \n",
    "            # early stopping increase\n",
    "            if epoch > early_stopping+1:\n",
    "                if train_costs[epoch] > train_costs[-early_stopping]:\n",
    "                    print('Stop at epoch %i. because training cost has increase in the last %i epochs.'% (\n",
    "                        epoch, early_stopping))\n",
    "                    \n",
    "                    parameters = old_param[-early_stopping]\n",
    "                    break\n",
    "                    \n",
    "                if test_costs[epoch] > test_costs[-early_stopping]:\n",
    "                    print('Stop at epoch %i. because validation cost has increase in the last %i epochs.'% (\n",
    "                        epoch, early_stopping))\n",
    "                    \n",
    "                    parameters = old_param[-early_stopping]\n",
    "                    break\n",
    "                    \n",
    "                \n",
    "        # Plot the cost\n",
    "        plot_cost(train_costs, test_costs)\n",
    "                \n",
    "        # lets save the parameters in a variable\n",
    "        del parameters['L'], old_param\n",
    "        parameters = sess.run(parameters)\n",
    "        print(\"Parameters have been trained!\")\n",
    "    \n",
    "    sess.close()\n",
    "    return parameters\n",
    "\n",
    "def neural_network(X_train, Y_train, X_test=None, Y_test=None,\n",
    "                   learning_rate=0.01, decay_step=1000, base_rate=0.95,\n",
    "                   num_epochs=None, epsilon=0.0001, early_stopping=10,\n",
    "                   n_units=[1], activation='relu', loss_fn='RMSE', \n",
    "                   optimizer_name='Adam', display_freq=100):\n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: 2 dimensions array like \n",
    "        array shape (n_rows, n_features)\n",
    "    Y_train: array like\n",
    "        array shape (n_rows, 1)\n",
    "    X_test: 2 dimensions array like (optional)\n",
    "        array shape (n_rows, n_features)\n",
    "    Y_test: array like (optional)\n",
    "        array shape (n_rows, 1)\n",
    "    learning_rate: float\n",
    "        Learning rate for the model\n",
    "    num_epochs: int (optional)\n",
    "        number of epoch to train\n",
    "        If None then it stop if the cost difference\n",
    "        is less than epsilon\n",
    "    epsilon: float (default 0.0001)\n",
    "        threshold to stop learning\n",
    "    early_stopping: int (default 10)\n",
    "        Number of epochs to use early stopping if training\n",
    "        cost increase\n",
    "    n_units: list (default [1])\n",
    "        list of units per layer\n",
    "    activation: str (default 'relu')\n",
    "        activation function for hidden layers\n",
    "    loss_fn: str (default RMSE)\n",
    "        loss function to compute\n",
    "    optimizer_name: str (default Adam)\n",
    "        optimizer to use\n",
    "    display_freq: int (default 100)\n",
    "        display frequency to follow the model learning\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict:\n",
    "        Dictionnary of the trained parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    if type(X_train) == pd.DataFrame:\n",
    "        X_train = X_train.values\n",
    "        Y_train = Y_train.values.reshape(-1,1)\n",
    "        \n",
    "    if X_test is not None:\n",
    "        if type(X_test) == pd.DataFrame:\n",
    "            X_test = X_test.values\n",
    "            Y_test = Y_test.values.reshape(-1,1)\n",
    "    \n",
    "    # Reset graph\n",
    "    tf.reset_default_graph()\n",
    "    # Set seed\n",
    "    tf.set_random_seed(42)\n",
    "    \n",
    "    _, n_features = X_train.shape\n",
    "    \n",
    "    # Initialize placeholder    \n",
    "    X = tf.placeholder(tf.float32, shape=(None, n_features), name='X')\n",
    "    Y = tf.placeholder(tf.float32, shape=(None, 1), name='Y')\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_units, n_features, n_units_output=1)\n",
    "    \n",
    "    # Create forward propagation graphs\n",
    "    Y_pred = forward_propagation(X, parameters, activation)\n",
    "    \n",
    "    # Create loss computation graph\n",
    "    loss = compute_loss(Y_pred, Y, loss=loss_fn)\n",
    "    \n",
    "#     decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps)  \n",
    "    \n",
    "    global_step = tf.Variable(0, trainable=False, name='learning_rate')\n",
    "    learning_rate = tf.train.exponential_decay(learning_rate,\n",
    "            global_step, decay_step, base_rate, staircase=True)\n",
    "\n",
    "    \n",
    "    # Create Optimizer \n",
    "    optimizer_obj = select_optimizer(optimizer_name)(learning_rate)\n",
    "    # grads_and_vars = optimizer_obj.compute_gradients(loss)    \n",
    "    optimizer = optimizer_obj.minimize(loss, global_step=global_step)\n",
    "    \n",
    "    feed_dict_train = {X: X_train, Y: Y_train}\n",
    "    feed_dict_test = {X: X_test, Y: Y_test} if X_test is not None else None\n",
    "    \n",
    "    parameters = fit(feed_dict_train, parameters, optimizer, loss, learning_rate,\n",
    "                     num_epochs, epsilon, early_stopping, feed_dict_test)\n",
    "    \n",
    "    return parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 0, learning rate is 0.01000000 train cost is: 11.426588 and test cost is: 11.382075\n",
      "At epoch 100, learning rate is 0.01000000 train cost is: 5.093220 and test cost is: 5.062395\n",
      "At epoch 200, learning rate is 0.01000000 train cost is: 3.759574 and test cost is: 3.730151\n",
      "At epoch 300, learning rate is 0.01000000 train cost is: 2.948740 and test cost is: 2.920679\n",
      "At epoch 400, learning rate is 0.01000000 train cost is: 2.361535 and test cost is: 2.335022\n",
      "At epoch 500, learning rate is 0.01000000 train cost is: 1.900781 and test cost is: 1.876168\n",
      "At epoch 600, learning rate is 0.01000000 train cost is: 1.523095 and test cost is: 1.500944\n",
      "At epoch 700, learning rate is 0.01000000 train cost is: 1.206378 and test cost is: 1.187585\n",
      "At epoch 800, learning rate is 0.01000000 train cost is: 0.939653 and test cost is: 0.925654\n",
      "At epoch 900, learning rate is 0.01000000 train cost is: 0.719832 and test cost is: 0.712876\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters have been trained!\n"
     ]
    }
   ],
   "source": [
    "# Choose hyper parameters\n",
    "learning_rate = 0.01\n",
    "decay_step = 1000 \n",
    "base_rate = 0.9\n",
    "num_epochs = 1000 \n",
    "epsilon = 0.000000001\n",
    "early_stopping = 10\n",
    "n_units = [20,10]\n",
    "activation = 'relu' \n",
    "loss_fn = 'RMSE log'\n",
    "optimizer = 'Adam'\n",
    "# batch_size to implement\n",
    "\n",
    "# Helper parameters\n",
    "display_freq = 100\n",
    "\n",
    "parameters = neural_network(X_train, y_train, X_test, y_test,\n",
    "                            learning_rate, decay_step, base_rate,\n",
    "                            num_epochs, epsilon, early_stopping,\n",
    "                            n_units, activation, loss_fn, optimizer,\n",
    "                            display_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard --logdir=\"./summaries/dashboard/\" --port 6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W0': array([[ 1.86939120e+00,  1.47575819e+00, -1.17285766e-01, ...,\n",
       "          1.44841208e-03,  1.13480043e+00, -1.59956619e-01],\n",
       "        [ 1.89776313e+00,  1.73469222e+00, -2.47178212e-01, ...,\n",
       "         -2.58718163e-01,  1.15031409e+00, -8.17708299e-02],\n",
       "        [ 1.99488580e+00,  1.60308421e+00,  1.51197836e-01, ...,\n",
       "         -1.34591803e-01,  1.04659402e+00, -2.23917052e-01],\n",
       "        ...,\n",
       "        [ 1.93920839e+00,  1.80467284e+00,  4.71154898e-02, ...,\n",
       "         -3.25532928e-02,  1.00585485e+00, -9.93239433e-02],\n",
       "        [ 2.09179163e+00,  1.74675488e+00, -2.03039035e-01, ...,\n",
       "         -1.43004328e-01,  1.11013472e+00, -1.19780332e-01],\n",
       "        [ 2.94279146e+00,  1.62525904e+00,  3.37335207e-02, ...,\n",
       "         -2.06520017e-02,  1.28375793e+00, -1.72843516e-01]], dtype=float32),\n",
       " 'b0': array([[ 2.02212   ,  1.7148827 , -0.06579651,  0.52811944,  0.99387693,\n",
       "          1.8723816 ,  4.429361  ,  0.74580085,  3.4721518 ,  0.6776308 ,\n",
       "          3.0988758 ,  0.8577727 ,  2.5664694 ,  1.7063924 , -0.07235721,\n",
       "          6.0429797 ,  4.0444756 , -0.0659229 ,  1.2913635 , -0.06172643]],\n",
       "       dtype=float32),\n",
       " 'W1': array([[ 2.667462  ,  5.597816  ,  5.2430296 , -0.18866192, -0.57137895,\n",
       "         -0.3603632 ,  3.0742297 , -0.13827056,  3.181968  ,  2.0940926 ],\n",
       "        [ 1.556272  ,  5.361939  ,  4.0289764 , -0.13960952, -0.34048408,\n",
       "         -0.22454181,  1.6575062 ,  0.12627187,  2.447125  ,  1.6831496 ],\n",
       "        [ 0.03273546,  0.10508275, -0.16016595, -0.41740316, -0.2783104 ,\n",
       "         -0.04769586, -0.21503805, -0.5011356 ,  0.3999382 , -0.07517323],\n",
       "        [ 1.2628971 ,  2.6777701 ,  2.4633188 ,  0.21239771,  0.27791566,\n",
       "         -0.38022327,  1.1733963 , -0.12484435,  0.818589  ,  1.1003251 ],\n",
       "        [ 1.2319778 ,  4.2536993 ,  2.8609202 , -0.25058338,  0.21686146,\n",
       "         -0.0483803 ,  1.5301895 ,  0.13421416,  2.238357  ,  1.582808  ],\n",
       "        [ 3.115924  ,  5.8215404 ,  4.982103  , -0.43304166,  0.26842713,\n",
       "         -0.39199525,  2.8152683 , -0.48017776,  3.601235  ,  2.622384  ],\n",
       "        [ 4.235431  ,  7.347064  ,  6.755554  ,  0.25487065, -0.45956033,\n",
       "          0.10934556,  4.014699  ,  0.01587526,  5.431443  ,  3.395412  ],\n",
       "        [ 1.492435  ,  3.6488273 ,  3.2480743 , -0.38368693,  0.04946985,\n",
       "         -0.42452985,  1.6719773 ,  0.15041724,  1.9011104 ,  1.4844245 ],\n",
       "        [ 3.1696463 ,  6.897777  ,  6.1131473 ,  0.07535227, -0.06964143,\n",
       "         -0.26546067,  3.7784328 , -0.28801635,  3.791565  ,  2.533724  ],\n",
       "        [ 1.5063198 ,  3.9830265 ,  2.8203764 , -0.19651662, -0.15794896,\n",
       "         -0.46566668,  1.5603056 , -0.38144648,  1.7958417 ,  1.2346513 ],\n",
       "        [ 4.27589   ,  6.2978287 ,  6.0542774 , -0.47956493, -0.51197344,\n",
       "         -0.4902351 ,  4.4137506 ,  0.1986722 ,  4.914495  ,  3.2470903 ],\n",
       "        [ 0.9051533 ,  3.8349054 ,  2.6385567 , -0.2974268 , -0.48244622,\n",
       "          0.3532665 ,  1.0916568 , -0.41020894,  1.5717522 ,  1.2991921 ],\n",
       "        [ 2.7890134 ,  5.7847834 ,  5.2327247 ,  0.32869697,  0.06013688,\n",
       "         -0.3803832 ,  2.6918478 , -0.26447284,  3.118221  ,  1.6444273 ],\n",
       "        [ 2.5501294 ,  5.7445445 ,  4.942399  , -0.21590228,  0.0542101 ,\n",
       "         -0.40462148,  2.6638153 , -0.01757985,  3.317039  ,  2.5014331 ],\n",
       "        [-0.35299036,  0.07605135,  0.20697129, -0.3376006 , -0.11806682,\n",
       "          0.02992678, -0.31933013, -0.31729233,  0.01703521,  0.16063344],\n",
       "        [ 6.044488  ,  7.506432  ,  7.18713   ,  0.20193452,  0.01374399,\n",
       "          0.3500636 ,  6.2002535 ,  0.18966904,  6.416139  ,  5.066945  ],\n",
       "        [ 4.5920467 ,  6.9532223 ,  6.7024965 , -0.04628045, -0.3280649 ,\n",
       "          0.13368484,  4.6384096 ,  0.16295576,  5.1044235 ,  3.359935  ],\n",
       "        [-0.15499312, -0.14104348, -0.08900104, -0.30393288,  0.07381034,\n",
       "          0.07390441, -0.26568207,  0.37807468,  0.29652128, -0.35312802],\n",
       "        [ 2.3788471 ,  5.121323  ,  3.8116088 , -0.12296697,  0.0851928 ,\n",
       "         -0.40523505,  1.8842232 , -0.02522766,  2.7295775 ,  1.9856097 ],\n",
       "        [ 0.07885707,  0.39314014,  0.3976151 ,  0.33634928,  0.17278981,\n",
       "         -0.05286154,  0.19416861, -0.2652777 , -0.10094297, -0.01469424]],\n",
       "       dtype=float32),\n",
       " 'b1': array([[ 0.23542176,  0.75363505,  0.45894867, -0.07400093, -0.09809467,\n",
       "         -0.06005454,  0.22189486, -0.06461857,  0.2683702 ,  0.19664821]],\n",
       "       dtype=float32),\n",
       " 'W2': array([[ 5.1559076 ],\n",
       "        [ 6.1099286 ],\n",
       "        [ 5.226003  ],\n",
       "        [-0.05391053],\n",
       "        [-0.04771347],\n",
       "        [-0.63869137],\n",
       "        [ 4.812475  ],\n",
       "        [-0.2545792 ],\n",
       "        [ 5.567934  ],\n",
       "        [ 4.027326  ]], dtype=float32),\n",
       " 'b2': array([[0.16265664]], dtype=float32)}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-use trained paramaters to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "    \"\"\"Returns predictions using trained parameters\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: pd.DataFrame or np.array\n",
    "        Features to predict on\n",
    "    parameters: dict\n",
    "        Trained parameters\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    np.array:\n",
    "        Computed predictions\n",
    "    \"\"\"\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    X_tf = tf.placeholder(tf.float32, shape=(None, X.shape[1]), name='X')\n",
    "    Y_pred = forward_propagation(X_tf, parameters)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        \n",
    "        preds = sess.run(Y_pred, feed_dict={X_tf: X})\n",
    "                   \n",
    "    sess.close()\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictions = predict(X_test, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[110608.49 ],\n",
       "       [115076.15 ],\n",
       "       [108816.65 ],\n",
       "       [112900.625],\n",
       "       [115287.89 ],\n",
       "       [106251.43 ],\n",
       "       [112564.35 ],\n",
       "       [114052.08 ],\n",
       "       [104584.48 ],\n",
       "       [108467.55 ]], dtype=float32)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "def custom_cost(Y_real, Y_pred):\n",
    "    return np.sqrt(np.mean((np.log(Y_real) - np.log(Y_pred))**2))\n",
    "\n",
    "def model_validation(X_train, Y_train, X_test, Y_test, parameters):\n",
    "    \"\"\"Compute and prints cost metrics \n",
    "    for the training and validation set\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: pd.DataFrame or np.array\n",
    "        Features for training set\n",
    "    Y_train: pd.Series or np.array\n",
    "        Labels for training set\n",
    "    X_test: pd.DataFrame or np.array\n",
    "        Features for testing set\n",
    "    Y_test: pd.Series or np.array\n",
    "        Labels for testing set\n",
    "    parameters: dict\n",
    "        Trained parameters    \n",
    "    \"\"\"\n",
    "    if type(X_train) == pd.DataFrame:\n",
    "        X_train = X_train.values\n",
    "        Y_train = Y_train.values.reshape(-1,1)\n",
    "        X_test = X_test.values\n",
    "        Y_test = Y_test.values.reshape(-1,1)    \n",
    "\n",
    "    pred_train = predict(X_train, parameters)\n",
    "    pred_test = predict(X_test, parameters)\n",
    "    \n",
    "    train_cost = custom_cost(Y_train, pred_train)\n",
    "    test_cost = custom_cost(Y_test, pred_test)\n",
    "\n",
    "    print (\"Train score:\", train_cost)\n",
    "    print (\"Test score:\", test_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.5522331239475746\n",
      "Test score: 0.5552056643844377\n"
     ]
    }
   ],
   "source": [
    "model_validation(X_train, y_train, X_test, y_test, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 0 ns, total: 6 µs\n",
      "Wall time: 11.2 µs\n",
      "At epoch 0, learning rate is 0.00050000 train cost is: 13.512259 and test cost is: 13.459157\n",
      "At epoch 500, learning rate is 0.00041405 train cost is: 0.349337 and test cost is: 0.390098\n",
      "At epoch 1000, learning rate is 0.00034287 train cost is: 0.230036 and test cost is: 0.251194\n",
      "At epoch 1500, learning rate is 0.00028393 train cost is: 0.203538 and test cost is: 0.218770\n",
      "At epoch 2000, learning rate is 0.00023513 train cost is: 0.193325 and test cost is: 0.208258\n",
      "At epoch 2500, learning rate is 0.00019471 train cost is: 0.186796 and test cost is: 0.201767\n",
      "At epoch 3000, learning rate is 0.00016124 train cost is: 0.181172 and test cost is: 0.196945\n",
      "At epoch 3500, learning rate is 0.00013352 train cost is: 0.176486 and test cost is: 0.193317\n",
      "At epoch 4000, learning rate is 0.00011057 train cost is: 0.172372 and test cost is: 0.190333\n",
      "At epoch 4500, learning rate is 0.00009156 train cost is: 0.168306 and test cost is: 0.187595\n",
      "At epoch 5000, learning rate is 0.00007582 train cost is: 0.164134 and test cost is: 0.185374\n",
      "At epoch 5500, learning rate is 0.00006279 train cost is: 0.159853 and test cost is: 0.183783\n",
      "At epoch 6000, learning rate is 0.00005200 train cost is: 0.155482 and test cost is: 0.183046\n",
      "Stop at epoch 6137. because validation cost has increase in the last 100 epochs.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters have been trained!\n",
      "Train score: 0.15425812206225104\n",
      "Test score: 0.183021221040079\n"
     ]
    }
   ],
   "source": [
    "%time \n",
    "\n",
    "# Choose hyper parameters\n",
    "learning_rate = 0.0005\n",
    "decay_step = 250\n",
    "base_rate = 0.91\n",
    "\n",
    "num_epochs = None \n",
    "epsilon = 0.000000001\n",
    "early_stopping = 100\n",
    "n_units = [90,50,70,30,40,20,10]\n",
    "activation = 'relu' \n",
    "loss_fn = 'RMSE log'\n",
    "optimizer = 'Adam'\n",
    "# batch_size to implement\n",
    "\n",
    "# Helper parameters\n",
    "display_freq = 500\n",
    "\n",
    "parameters = neural_network(X_train, y_train, X_test, y_test,\n",
    "                           learning_rate, decay_step, base_rate,\n",
    "                            num_epochs, epsilon, early_stopping,\n",
    "                           n_units, activation, loss_fn, optimizer,\n",
    "                           display_freq)\n",
    "\n",
    "# see model validation\n",
    "model_validation(X_train, y_train, X_test, y_test, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on test and save submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_and_format_for_kaggle(test, parameters, save=True):\n",
    "    ids = test['Id']\n",
    "    X = test.drop(columns='Id')\n",
    "    \n",
    "    preds = pd.DataFrame(predict(X, parameters))\n",
    "    \n",
    "    output = pd.concat([ids, preds], axis=1)\n",
    "    output.columns = ['Id','SalePrice']\n",
    "    \n",
    "    if save:\n",
    "        output.to_csv('data/submission.csv', index=False)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_columns = [c for c in cols_to_keep if c != target]\n",
    "test_df, _ , _ = prepare_data(test, test_columns, target, \n",
    "                              dummies_cols, transformer)\n",
    "test_df['Id'] = test['Id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = pred_and_format_for_kaggle(test_df, parameters, save=True)\n",
    "(output['SalePrice'] < 1000).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1087</th>\n",
       "      <td>2548</td>\n",
       "      <td>124962.890625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345</th>\n",
       "      <td>2806</td>\n",
       "      <td>115071.953125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>1797</td>\n",
       "      <td>112603.890625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>1744</td>\n",
       "      <td>375710.843750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>2151</td>\n",
       "      <td>140233.578125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>2755</td>\n",
       "      <td>132624.390625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>2350</td>\n",
       "      <td>332385.156250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>1899</td>\n",
       "      <td>186297.515625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042</th>\n",
       "      <td>2503</td>\n",
       "      <td>79788.304688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>2177</td>\n",
       "      <td>226972.078125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id      SalePrice\n",
       "1087  2548  124962.890625\n",
       "1345  2806  115071.953125\n",
       "336   1797  112603.890625\n",
       "283   1744  375710.843750\n",
       "690   2151  140233.578125\n",
       "1294  2755  132624.390625\n",
       "889   2350  332385.156250\n",
       "438   1899  186297.515625\n",
       "1042  2503   79788.304688\n",
       "716   2177  226972.078125"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The end."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
