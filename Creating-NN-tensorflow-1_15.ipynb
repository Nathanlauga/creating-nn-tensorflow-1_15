{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a basic neural network with Tensorflow v1.15\n",
    "\n",
    "This notebook is inspired by some code available in the [Coursera Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning#courses) and has the goal to reproduce the same logic as [Tensorflow playground](https://playground.tensorflow.org/) but in Python.\n",
    "\n",
    "So in summary you'll be able to choose the following hyperparameters :\n",
    "- number of hidden layers\n",
    "- number of units in each layers\n",
    "- loss function\n",
    "- activation per layer\n",
    "- optimizer to use\n",
    "- learning rate\n",
    "- batch size\n",
    "\n",
    "## Data used\n",
    "\n",
    "This notebook's goal is to create a few neural networks with `tensorflow 1.15` and try to submit the result into Kaggle.\n",
    "\n",
    "[Competition link](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/).\n",
    "\n",
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check GPU is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 2172091511010242996),\n",
       " _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 12033409450800053158),\n",
       " _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 7467996180272201805),\n",
       " _DeviceAttributes(/job:localhost/replica:0/task:0/device:GPU:0, GPU, 1282539520, 293092288534273855)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    devices = sess.list_devices()\n",
    "sess.close()\n",
    "\n",
    "devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available(\n",
    "    cuda_only=False, min_cuda_compute_capability=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 81)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'SalePrice'\n",
    "cols_to_keep = ['LotArea', 'TotalBsmtSF'] + [target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train[cols_to_keep].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = train_df.drop(columns=target), train_df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create NN\n",
    "\n",
    "### Initialize Weight and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_units, n_features, n_units_output=1):    \n",
    "    \"\"\"Initialize parameters Weight and bias and stored them\n",
    "    in a dictionnary following this format : {'Wl': tf.Variable, 'bl': tf.Variable, ...}.\n",
    "    \n",
    "    Where `l` is the layer number. \n",
    "    \n",
    "    The dictionnary also store number of layers L (output layer include)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_units: list\n",
    "        list of number of units per layer\n",
    "    n_features: int\n",
    "        number of features at input layer\n",
    "    n_units_output: int (default 1)\n",
    "        number of units at output layer\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict:\n",
    "        Dictionnary of parameters    \n",
    "    \"\"\"\n",
    "    tf.set_random_seed(1) \n",
    "    parameters = {}\n",
    "\n",
    "    n_units += [n_units_output]    \n",
    "    n_layers = len(n_units)\n",
    "    \n",
    "    parameters['L'] = n_layers\n",
    "    \n",
    "    for l in range(0,n_layers):\n",
    "        num_units = n_units[l]   \n",
    "        W = f'W{l}'\n",
    "        b = f'b{l}'\n",
    "\n",
    "        prev_n_units = n_features if l == 0 else n_units[l-1]\n",
    "\n",
    "        # Using Xavier initializer for Weight \n",
    "        # Todo custom\n",
    "        parameters[W] = tf.get_variable(name=W, shape=[prev_n_units, num_units], \n",
    "                                        initializer = tf.contrib.layers.xavier_initializer())\n",
    "        # Using zeros initializer for bias\n",
    "        parameters[b] = tf.get_variable(name=b, shape=[1,num_units], \n",
    "                                        initializer = tf.zeros_initializer())\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'L': 3,\n",
       " 'W0': <tf.Variable 'W0:0' shape=(20, 10) dtype=float32_ref>,\n",
       " 'b0': <tf.Variable 'b0:0' shape=(1, 10) dtype=float32_ref>,\n",
       " 'W1': <tf.Variable 'W1:0' shape=(10, 5) dtype=float32_ref>,\n",
       " 'b1': <tf.Variable 'b1:0' shape=(1, 5) dtype=float32_ref>,\n",
       " 'W2': <tf.Variable 'W2:0' shape=(5, 2) dtype=float32_ref>,\n",
       " 'b2': <tf.Variable 'b2:0' shape=(1, 2) dtype=float32_ref>}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test for a NN with 2 hidden layers, 20 features and 2 output units\n",
    "tf.reset_default_graph()   \n",
    "\n",
    "initialize_parameters(n_units=[10,5], n_features=20, n_units_output=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_layer(A, parameters, layer, activation='relu'):\n",
    "    \"\"\"Compute calculus for a given layer\n",
    "    \n",
    "    Follow this formula : Z_l = A_{l-1} * W_l + b_l\n",
    "    \n",
    "    if activation is set then this function returns : \n",
    "    A_l = activation_function(Z_l) \n",
    "    \n",
    "    Where :\n",
    "    - A is (1, nb units prev layer)\n",
    "    - W is (nb units prev layer, nb units current layer)\n",
    "    - b is (1, nb units current layer)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    A: tf.Variable\n",
    "        output of the previous layer\n",
    "    parameters: dict\n",
    "        Dictionnary of parameters  \n",
    "    layer: int\n",
    "        number of the current layer\n",
    "    activation: str (default relu)\n",
    "        activation function, if None then just \n",
    "        compute the linear calculus\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tf.Variable\n",
    "        Output of the layer calculus\n",
    "    \"\"\"\n",
    "    W = parameters['W'+str(layer)]\n",
    "    b = parameters['b'+str(layer)]\n",
    "    \n",
    "    Z = tf.add(tf.matmul(A,W), b, name=f'Z{layer}')\n",
    "    \n",
    "    if activation is not None:\n",
    "        return tf.nn.relu(Z, name=f'A{layer}') \n",
    "    \n",
    "    return Z\n",
    "\n",
    "def forward_propagation(X, parameters, activation='relu'):\n",
    "    \"\"\"Compute forward propagation given input X and\n",
    "    initialized parameters.\n",
    "    \n",
    "    You can custom activation functionfor the hidden layers\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: tf.Variable\n",
    "        input data shape (n_rows, n_features) \n",
    "    parameters: dict\n",
    "        Dictionnary of parameters  \n",
    "    activation: str (default relu)\n",
    "        activation function for hidden layers\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tf.Variable\n",
    "        Prediction output shape (n_rows, n_units_output)\n",
    "    \"\"\"\n",
    "    A = X\n",
    "    if 'L' not in parameters:\n",
    "        n_layers = int(len(parameters) / 2)\n",
    "    else:\n",
    "        n_layers = parameters['L']\n",
    "    \n",
    "    for l in range(0, n_layers):\n",
    "        act_l = activation if l != n_layers-1 else None\n",
    "        \n",
    "        A = compute_layer(A, parameters, layer=l, activation=act_l)\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Z2:0' shape=(?, 2) dtype=float32>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test for a NN with 2 hidden layers, 20 features and 2 output units\n",
    "tf.reset_default_graph()   \n",
    "\n",
    "parameters = initialize_parameters(n_units=[10,5], n_features=20, n_units_output=2)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, 20), name='X')\n",
    "\n",
    "forward_propagation(X, parameters, activation='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute cost\n",
    "\n",
    "Handle the following cost function :\n",
    "\n",
    "-----\n",
    "\n",
    "- `MSE`: Mean square Error (or Quadratic Loss or L2 Loss)\n",
    "\n",
    "$MSE = \\frac{1}{n}\\sum_{i=1}^{n}{(y_i - \\hat{y_i})^2}$\n",
    "\n",
    "```python\n",
    "def loss(Y_pred, Y_real):\n",
    "    return tf.reduce_mean(tf.square(Y_pred - Y_real))\n",
    "```\n",
    "\n",
    "-----\n",
    "- `RMSE`: Root Mean square Error \n",
    "\n",
    "$RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}{(y_i - \\hat{y_i})^2}}$\n",
    "\n",
    "```python\n",
    "def loss(Y_pred, Y_real):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(Y_pred - Y_real)))\n",
    "```\n",
    "-----\n",
    "- `RMSE log`: Root Mean square Error (log values)\n",
    "\n",
    "$RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}{(\\log{y_i} - \\log{\\hat{y_i}})^2}}$\n",
    "\n",
    "```python\n",
    "def loss(Y_pred, Y_real):\n",
    "    return tf.reduce_mean(tf.abs(Y_pred - Y_real))\n",
    "```\n",
    "\n",
    "-----\n",
    "- `MAE`: Mean Absolute Error (or L1 Loss)\n",
    "\n",
    "$MAE = \\frac{1}{n} \\sum_{i=1}^{n}{|y_i - \\hat{y_i}|}$\n",
    "\n",
    "```python\n",
    "def loss(Y_pred, Y_real):\n",
    "    return tf.reduce_mean(tf.abs(Y_pred - Y_real))\n",
    "```\n",
    "\n",
    "-----\n",
    "- `Entropy`: Cross Entropy Loss (or Negative Log Likelihood)\n",
    "\n",
    "$\\text{Cross Entropy Loss} = - (y_i\\log{\\hat{y_i}} + (1-y_i)\\log{1-\\hat{y_i}})$\n",
    "\n",
    "```python\n",
    "def loss(Y_pred, Y_real):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels, logits))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(Y_pred, Y_real, loss='RMSE'):\n",
    "    \"\"\"Compute loss given a loss function\n",
    "    \n",
    "    Loss available : \n",
    "    - MSE\n",
    "    - RMSE\n",
    "    - RMSE log\n",
    "    - MAE\n",
    "    - Entropy\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Y_pred: tf.Variable\n",
    "        Predictions from the model\n",
    "    Y_real: tf.Variable\n",
    "        Real output\n",
    "    loss: str (default RMSE)\n",
    "        loss function to compute\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tf.Variable:\n",
    "        Loss of the model\n",
    "    \"\"\"\n",
    "    if loss == 'MSE':\n",
    "        return tf.reduce_mean(tf.square(Y_pred - Y_real), name='loss')\n",
    "    \n",
    "    elif loss == 'RMSE':\n",
    "        return tf.sqrt(tf.reduce_mean(tf.square(Y_pred - Y_real)), name='loss')\n",
    "    \n",
    "    elif loss == 'RMSE log':\n",
    "        return tf.reduce_mean(tf.abs(tf.log(Y_pred) - tf.log(Y_real)), name='loss')\n",
    "        \n",
    "    elif loss == 'MAE':\n",
    "        return tf.reduce_mean(tf.abs(Y_pred - Y_real), name='loss')\n",
    "    \n",
    "    elif loss == 'Entropy':\n",
    "        return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(Y_real, Y_pred), name='loss')\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "tf.reset_default_graph()   \n",
    "\n",
    "parameters = initialize_parameters(n_units=[10], n_features=5)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, 5), name='X')\n",
    "Y = tf.placeholder(tf.float32, shape=(None, 1), name='Y')\n",
    "\n",
    "Y_pred = forward_propagation(X, parameters, activation='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'loss:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss(Y_pred=Y_pred, Y_real=Y, loss='MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'loss_1:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss(Y_pred=Y_pred, Y_real=Y, loss='RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'loss_2:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss(Y_pred=Y_pred, Y_real=Y, loss='RMSE log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'loss_3:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss(Y_pred=Y_pred, Y_real=Y, loss='MAE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'loss_4:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss(Y_pred=Y_pred, Y_real=Y, loss='Entropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer selection\n",
    "\n",
    "This the list of possible optimizer to train the neural network :\n",
    "\n",
    "- GradientDescent: [`tf.train.GradientDescentOptimizer`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/train/GradientDescentOptimizer)\n",
    "- Momentum: [`tf.train.MomentumOptimizer`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/train/MomentumOptimizer)\n",
    "- Adagrad: [`tf.train.AdagradOptimizer`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/train/AdagradOptimizer)\n",
    "- Adadelta: [`tf.train.AdadeltaOptimizer`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/train/AdadeltaOptimizer)\n",
    "- RMSProp: [`tf.train.RMSPropOptimizer`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/train/RMSPropOptimizer)\n",
    "- Adam: [`tf.train.AdamOptimizer`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/train/AdamOptimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_optimizer(optimizer='GradientDescent'):\n",
    "    \"\"\"Select the optimizer\n",
    "    \n",
    "    Available optimizers:\n",
    "    - GradientDescent\n",
    "    - Momentum\n",
    "    - Adagrad\n",
    "    - Adadelta\n",
    "    - RMSProp\n",
    "    - Adam\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    optimizer: str (default GradientDescent)\n",
    "        optimizer to select\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tf.train.Optimizer\n",
    "        Optimizer object\n",
    "    \"\"\"\n",
    "    if optimizer == 'GradientDescent':\n",
    "        return tf.train.GradientDescentOptimizer\n",
    "    elif optimizer == 'Momentum':\n",
    "        return tf.train.MomentumOptimizer\n",
    "    elif optimizer == 'Adagrad':\n",
    "        return tf.train.AdagradOptimizer\n",
    "    elif optimizer == 'Adadelta':\n",
    "        return tf.train.AdadeltaOptimizer\n",
    "    elif optimizer == 'RMSProp':\n",
    "        return tf.train.RMSPropOptimizer\n",
    "    elif optimizer == 'Adam':\n",
    "        return tf.train.AdamOptimizer\n",
    "    \n",
    "    raise ValueError('Optimizer choosen not available please select a valid one')\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.training.adam.AdamOptimizer"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_optimizer(optimizer='Adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensorboard(sess, L, grads_and_vars, loss, name='dashboard'):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if not os.path.exists('summaries'):\n",
    "        os.mkdir('summaries')\n",
    "    if not os.path.exists(os.path.join('summaries',name)):\n",
    "        os.mkdir(os.path.join('summaries',name))\n",
    "\n",
    "    summ_writer = tf.summary.FileWriter(os.path.join('summaries',name), sess.graph)    \n",
    "    \n",
    "    # tf_loss_summary :     you feed in a value by means of a placeholder, \n",
    "    #                       whenever you need to publish this to the board\n",
    "    \n",
    "    with tf.name_scope('performance'):\n",
    "        tf_loss_ph = tf.placeholder(tf.float32,shape=None,name='loss_summary')\n",
    "        tf_loss_summary = tf.summary.scalar('loss', tf_loss_ph)\n",
    "\n",
    "        \n",
    "    # tf_gradnorm_summary : this calculates the l2 norm of the gradients \n",
    "    #                       of the last layer of your neural network. \n",
    "    #                       Gradient norm is a good indicator of whether the weights \n",
    "    #                       of the neural network are being properly updated. \n",
    "    #                       A too small gradient norm can indicate vanishing gradient \n",
    "    #                       or a too large gradient can imply exploding gradient phenomenon.\n",
    "    \n",
    "    for g, v in grads_and_vars:\n",
    "        if f'W{L-1}' in v.name:\n",
    "            with tf.name_scope('gradients'):\n",
    "                tf_last_grad_norm = tf.sqrt(tf.reduce_mean(g**2))\n",
    "                tf_gradnorm_summary = tf.summary.scalar('grad_norm', tf_last_grad_norm)\n",
    "                break\n",
    "                \n",
    "    performance_summaries = tf.summary.merge([tf_loss_summary])    \n",
    "    \n",
    "    return {\n",
    "        'writer': summ_writer, \n",
    "        'loss': tf_loss_ph, \n",
    "        'gradnorm': tf_gradnorm_summary,\n",
    "        'perf': performance_summaries\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate NN with optimizer for back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cost(train_costs, test_costs=None):\n",
    "    \"\"\"Plot the cost\n",
    "    \"\"\"\n",
    "    plt.plot(np.squeeze(train_costs), label='train')\n",
    "    if test_costs is not None:\n",
    "        plt.plot(np.squeeze(test_costs), label='test')\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per fives)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def fit(feed_dict_train, parameters, optimizer, loss, num_epochs,\n",
    "        epsilon, early_stopping, feed_dict_test, batch_size=None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # TODO : implement batch size\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    train_costs = list()\n",
    "    test_costs = list()\n",
    "    num_epochs = 100000000 if num_epochs is None else num_epochs\n",
    "    L = parameters['L']\n",
    "    old_param = list()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        \n",
    "        # board = create_tensorboard(sess, L, grads_and_vars, loss, name='dashboard')\n",
    "        # summ_writer = board['writer']\n",
    "        # tf_loss_ph = board['loss']\n",
    "        # tf_gradnorm_summary = board['gradnorm']\n",
    "        # performance_summaries = board['perf']\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            sess.run(optimizer, feed_dict=feed_dict_train)\n",
    "            \n",
    "            # epoch_l_train, gn_summ = sess.run([loss, tf_gradnorm_summary], feed_dict=feed_dict_train)\n",
    "            epoch_l_train = sess.run(loss, feed_dict=feed_dict_train)\n",
    "            epoch_l_test = sess.run(loss, feed_dict=feed_dict_test)\n",
    "            \n",
    "            # summ = sess.run(performance_summaries, feed_dict={tf_loss_ph: epoch_l_train})\n",
    "            \n",
    "            # summ_writer.add_summary(gn_summ, epoch)\n",
    "            # summ_writer.add_summary(summ, epoch)\n",
    "\n",
    "            if epoch % display_freq == 0:\n",
    "                print (\"At epoch %i, train cost is: %f and test cost is: %f\" % (\n",
    "                    epoch, epoch_l_train, epoch_l_test))\n",
    "            \n",
    "            train_costs.append(epoch_l_train)\n",
    "            test_costs.append(epoch_l_test)\n",
    "                \n",
    "            # Early stopping slow decrease\n",
    "            if len(train_costs) > 2:\n",
    "                if train_costs[-2] - train_costs[-1] < epsilon:\n",
    "                    print('Stop at epoch %i. because cost decrease is less than epsilon.'% (epoch))\n",
    "                    break\n",
    "                        \n",
    "            old_param.append(parameters)\n",
    "            \n",
    "            # early stopping increase\n",
    "            if epoch > early_stopping+1:\n",
    "                if train_costs[epoch] > train_costs[-early_stopping]:\n",
    "                    print('Stop at epoch %i. because cost has increase in the last %i epochs.'% (\n",
    "                        epoch, early_stopping))\n",
    "                    \n",
    "                    parameters = old_param[-early_stopping]\n",
    "                    break\n",
    "                    \n",
    "                \n",
    "        # Plot the cost\n",
    "        plot_cost(train_costs, test_costs)\n",
    "                \n",
    "        # lets save the parameters in a variable\n",
    "        del parameters['L'], old_param\n",
    "        parameters = sess.run(parameters)\n",
    "        print(\"Parameters have been trained!\")\n",
    "    \n",
    "    sess.close()\n",
    "    return parameters\n",
    "\n",
    "def neural_network(X_train, Y_train, X_test=None, Y_test=None,\n",
    "                   learning_rate=0.01, num_epochs=None, epsilon=0.0001, early_stopping=10,\n",
    "                   n_units=[1], activation='relu', loss_fn='RMSE', \n",
    "                   optimizer_name='Adam', display_freq=100):\n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: 2 dimensions array like \n",
    "        array shape (n_rows, n_features)\n",
    "    Y_train: array like\n",
    "        array shape (n_rows, 1)\n",
    "    X_test: 2 dimensions array like (optional)\n",
    "        array shape (n_rows, n_features)\n",
    "    Y_test: array like (optional)\n",
    "        array shape (n_rows, 1)\n",
    "    learning_rate: float\n",
    "        Learning rate for the model\n",
    "    num_epochs: int (optional)\n",
    "        number of epoch to train\n",
    "        If None then it stop if the cost difference\n",
    "        is less than epsilon\n",
    "    epsilon: float (default 0.0001)\n",
    "        threshold to stop learning\n",
    "    early_stopping: int (default 10)\n",
    "        Number of epochs to use early stopping if training\n",
    "        cost increase\n",
    "    n_units: list (default [1])\n",
    "        list of units per layer\n",
    "    activation: str (default 'relu')\n",
    "        activation function for hidden layers\n",
    "    loss_fn: str (default RMSE)\n",
    "        loss function to compute\n",
    "    optimizer_name: str (default Adam)\n",
    "        optimizer to use\n",
    "    display_freq: int (default 100)\n",
    "        display frequency to follow the model learning\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict:\n",
    "        Dictionnary of the trained parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    if type(X_train) == pd.DataFrame:\n",
    "        X_train = X_train.values\n",
    "        Y_train = Y_train.values.reshape(-1,1)\n",
    "        \n",
    "    if X_test is not None:\n",
    "        if type(X_test) == pd.DataFrame:\n",
    "            X_test = X_test.values\n",
    "            Y_test = Y_test.values.reshape(-1,1)\n",
    "    \n",
    "    # Reset graph\n",
    "    tf.reset_default_graph()\n",
    "    # Set seed\n",
    "    tf.set_random_seed(42)\n",
    "    \n",
    "    _, n_features = X_train.shape\n",
    "    \n",
    "    # Initialize placeholder    \n",
    "    X = tf.placeholder(tf.float32, shape=(None, n_features), name='X')\n",
    "    Y = tf.placeholder(tf.float32, shape=(None, 1), name='Y')\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_units, n_features, n_units_output=1)\n",
    "    \n",
    "    # Create forward propagation graphs\n",
    "    Y_pred = forward_propagation(X, parameters, activation)\n",
    "    \n",
    "    # Create loss computation graph\n",
    "    loss = compute_loss(Y_pred, Y, loss=loss_fn)\n",
    "    \n",
    "    # Create Optimizer \n",
    "    optimizer_obj = select_optimizer(optimizer_name)(learning_rate)\n",
    "    # grads_and_vars = optimizer_obj.compute_gradients(loss)    \n",
    "    optimizer = optimizer_obj.minimize(loss)\n",
    "    \n",
    "    feed_dict_train = {X: X_train, Y: Y_train}\n",
    "    feed_dict_test = {X: X_test, Y: Y_test} if X_test is not None else None\n",
    "    \n",
    "    parameters = fit(feed_dict_train, parameters, optimizer, loss, \n",
    "                     num_epochs, epsilon, early_stopping, feed_dict_test)\n",
    "    \n",
    "    return parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 0, train cost is: 3.671576 and test cost is: 3.692336\n",
      "At epoch 1000, train cost is: 0.339961 and test cost is: 0.380016\n",
      "At epoch 2000, train cost is: 0.325864 and test cost is: 0.363731\n",
      "At epoch 3000, train cost is: 0.300271 and test cost is: 0.336007\n",
      "At epoch 4000, train cost is: 0.263493 and test cost is: 0.291330\n",
      "Stop at epoch 4521. because cost decrease is less than epsilon.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters have been trained!\n"
     ]
    }
   ],
   "source": [
    "# Choose hyper parameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = None \n",
    "epsilon = 0.0000001\n",
    "early_stopping = 10\n",
    "n_units = [20,10]\n",
    "activation = 'relu' \n",
    "loss_fn = 'RMSE log'\n",
    "optimizer = 'Adam'\n",
    "# batch_size to implement\n",
    "\n",
    "# Helper parameters\n",
    "display_freq = 1000\n",
    "\n",
    "parameters = neural_network(X_train, y_train, X_test, y_test,\n",
    "                           learning_rate, num_epochs, epsilon, early_stopping,\n",
    "                           n_units, activation, loss_fn, optimizer,\n",
    "                           display_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard --logdir=\"./summaries/dashboard/\" --port 6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W0': array([[-0.31792915, -0.4910443 , -0.1410751 , -0.254693  ,  0.42514408,\n",
       "          0.30987385, -0.46448395,  0.27176884,  0.32814178,  0.438562  ,\n",
       "          0.54065424,  0.26098844,  0.36277163,  0.4135872 ,  0.16505486,\n",
       "         -0.20105085, -0.00830662,  0.7644946 , -0.32265282, -0.25766718],\n",
       "        [-0.29599485,  0.03434199, -0.47453353, -0.4941913 ,  1.9638524 ,\n",
       "          1.6521082 , -0.39467865,  1.8534039 ,  2.8875992 ,  1.2206137 ,\n",
       "          1.5388526 ,  2.112493  ,  2.5215755 ,  1.1070802 ,  0.38834536,\n",
       "          0.22533303, -0.13793387, -1.8639503 , -0.41960865, -0.05248371]],\n",
       "       dtype=float32),\n",
       " 'b0': array([[ 0.        ,  0.        ,  0.        ,  0.        ,  1.696772  ,\n",
       "          2.000338  ,  0.        ,  1.7824246 ,  2.6830742 ,  1.6107584 ,\n",
       "          1.7870563 ,  1.9979225 ,  2.4823978 ,  1.6945149 ,  0.35883093,\n",
       "          0.        , -0.00623414, -1.8658538 ,  0.        ,  0.        ]],\n",
       "       dtype=float32),\n",
       " 'W1': array([[ 0.02947131, -0.22932696,  0.29630804, -0.09849054, -0.44012812,\n",
       "         -0.3003166 ,  0.37081802, -0.06010097, -0.15104839, -0.03013247],\n",
       "        [-0.21542308,  0.21826804, -0.02534273, -0.05927294, -0.22311088,\n",
       "         -0.16448835, -0.11930716,  0.19382095,  0.12244606,  0.33814508],\n",
       "        [-0.03039286,  0.04198086, -0.22312433, -0.3541385 , -0.21796627,\n",
       "          0.01204553, -0.27819127, -0.44162256,  0.33727902, -0.13781592],\n",
       "        [ 0.3980723 , -0.29534814,  0.3604256 ,  0.2920977 ,  0.39077258,\n",
       "         -0.32017094,  0.3062725 , -0.05742621, -0.33065593,  0.4095286 ],\n",
       "        [ 0.26733392,  0.8071632 ,  0.20575927, -0.17082107,  0.28558993,\n",
       "          0.01167247,  0.4587259 ,  0.17125587,  0.79945934,  0.8067062 ],\n",
       "        [ 0.9115756 ,  1.118147  ,  0.9156337 , -0.33957672,  0.34384948,\n",
       "         -0.39199525,  0.4598361 , -0.437601  ,  0.6920976 ,  0.79343367],\n",
       "        [ 0.02762756,  0.23020583,  0.32225174,  0.32885647, -0.34567168,\n",
       "          0.16938955, -0.27224228,  0.07860667,  0.42711955, -0.11021587],\n",
       "        [ 0.75783545,  0.80619013,  1.0908482 , -0.30094492,  0.09115987,\n",
       "         -0.36448282,  0.7953171 ,  0.18625063,  0.69680977,  0.8635798 ],\n",
       "        [ 0.5935599 ,  1.4364887 ,  1.2942073 ,  0.16476196, -0.03631043,\n",
       "         -0.20541161,  0.9942351 , -0.24391064,  0.29421717,  0.4022458 ],\n",
       "        [ 0.66202694,  0.9001532 ,  0.5004821 , -0.11242983, -0.04892328,\n",
       "         -0.40561965,  0.61198264, -0.34164712,  0.5024461 ,  0.53436625],\n",
       "        [ 0.69614524,  0.20357546,  0.5264888 , -0.3832206 , -0.39226353,\n",
       "         -0.4303078 ,  0.66966724,  0.27273634,  0.53791213,  0.17278364],\n",
       "        [ 0.40924042,  1.0572426 ,  0.6905619 , -0.21697286, -0.4499986 ,\n",
       "          0.41332048,  0.48021218, -0.37509036,  0.6438316 ,  0.9002715 ],\n",
       "        [ 0.99913704,  0.92049557,  1.1569856 ,  0.41297013,  0.10600056,\n",
       "         -0.3203455 ,  0.7057591 , -0.22874388,  0.48175082,  0.18665011],\n",
       "        [ 0.36579162,  0.8670154 ,  0.779242  , -0.12632027,  0.18555033,\n",
       "         -0.34504163,  0.34890318,  0.0327923 ,  0.46082327,  0.70167124],\n",
       "        [-0.1982715 ,  0.2983252 ,  0.3673345 , -0.27524525, -0.08247858,\n",
       "          0.08997607, -0.23016797, -0.28392407,  0.1101814 ,  0.24577501],\n",
       "        [ 0.19529045, -0.4146098 , -0.25384283,  0.26496273,  0.08406013,\n",
       "          0.41011494,  0.27526367,  0.25237292, -0.08567572, -0.20080781],\n",
       "        [ 0.26890773, -0.08978941,  0.31148908,  0.05939931, -0.20059486,\n",
       "          0.1933406 ,  0.2402293 ,  0.25696647,  0.0293262 , -0.3020603 ],\n",
       "        [-1.6790757 , -2.6121721 , -1.93601   , -0.24283484,  0.47765672,\n",
       "          0.13381112, -1.1179568 ,  0.40980595, -0.60987073, -1.0895668 ],\n",
       "        [ 0.33703005,  0.26467752, -0.20081261, -0.03360596,  0.21353453,\n",
       "         -0.34518442, -0.21163803,  0.04223794,  0.15561849,  0.33675528],\n",
       "        [ 0.00829491,  0.31976098,  0.32631612,  0.4020521 ,  0.2422002 ,\n",
       "         -0.05286154,  0.12388915, -0.2043939 , -0.17348102, -0.08476886]],\n",
       "       dtype=float32),\n",
       " 'b1': array([[ 1.0892674 ,  1.8748151 ,  1.4589231 ,  0.        , -0.27749404,\n",
       "          0.        ,  0.7656286 , -0.0019087 ,  0.8047421 ,  0.6854911 ]],\n",
       "       dtype=float32),\n",
       " 'W2': array([[ 1.5469625 ],\n",
       "        [ 0.97563374],\n",
       "        [ 0.8825176 ],\n",
       "        [-0.12535512],\n",
       "        [-0.27639064],\n",
       "        [-0.6987406 ],\n",
       "        [ 1.1694103 ],\n",
       "        [-0.3204602 ],\n",
       "        [ 0.96873575],\n",
       "        [ 1.2677459 ]], dtype=float32),\n",
       " 'b2': array([[0.49177718]], dtype=float32)}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-use trained paramaters to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "    \"\"\"Returns predictions using trained parameters\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: pd.DataFrame or np.array\n",
    "        Features to predict on\n",
    "    parameters: dict\n",
    "        Trained parameters\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    np.array:\n",
    "        Computed predictions\n",
    "    \"\"\"\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    X_tf = tf.placeholder(tf.float32, shape=(None, X.shape[1]), name='X')\n",
    "    Y_pred = forward_propagation(X_tf, parameters)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        \n",
    "        preds = sess.run(Y_pred, feed_dict={X_tf: X})\n",
    "                   \n",
    "    sess.close()\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictions = predict(X_test, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[163747.56],\n",
       "       [230627.8 ],\n",
       "       [162568.89],\n",
       "       [134273.81],\n",
       "       [250622.1 ],\n",
       "       [ 72323.31],\n",
       "       [189081.28],\n",
       "       [100426.89],\n",
       "       [ 75831.37],\n",
       "       [196533.86]], dtype=float32)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "def model_validation(X_train, Y_train, X_test, Y_test, parameters):\n",
    "    \"\"\"Compute and prints cost metrics \n",
    "    for the training and validation set\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: pd.DataFrame or np.array\n",
    "        Features for training set\n",
    "    Y_train: pd.Series or np.array\n",
    "        Labels for training set\n",
    "    X_test: pd.DataFrame or np.array\n",
    "        Features for testing set\n",
    "    Y_test: pd.Series or np.array\n",
    "        Labels for testing set\n",
    "    parameters: dict\n",
    "        Trained parameters    \n",
    "    \"\"\"\n",
    "    if type(X_train) == pd.DataFrame:\n",
    "        X_train = X_train.values\n",
    "        Y_train = Y_train.values.reshape(-1,1)\n",
    "        X_test = X_test.values\n",
    "        Y_test = Y_test.values.reshape(-1,1)    \n",
    "\n",
    "    pred_train = predict(X_train, parameters)\n",
    "    pred_test = predict(X_test, parameters)\n",
    "    \n",
    "    train_cost = np.sqrt(mean_squared_log_error(Y_train, pred_train ))\n",
    "    test_cost = np.sqrt(mean_squared_log_error(Y_test, pred_test ))\n",
    "\n",
    "    print (\"Train score:\", train_cost)\n",
    "    print (\"Test score:\", test_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.3347047369085436\n",
      "Test score: 0.35267985293589066\n"
     ]
    }
   ],
   "source": [
    "model_validation(X_train, y_train, X_test, y_test, parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
