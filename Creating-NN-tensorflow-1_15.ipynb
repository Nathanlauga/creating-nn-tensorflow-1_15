{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a basic neural network with Tensorflow v1.15\n",
    "\n",
    "This notebook is inspired by some code available in the [Coursera Deep Learning Specialization]() and has the goal to reproduce the same logic as [Tensorflow playground](https://playground.tensorflow.org/) but in Python.\n",
    "\n",
    "So in summary you'll be able to choose the following hyperparameters :\n",
    "- number of hidden layers\n",
    "- number of units in each layers\n",
    "- loss function\n",
    "- activation per layer\n",
    "- regularization (with rate)\n",
    "- learning rate\n",
    "\n",
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'SalePrice'\n",
    "cols_to_keep = ['LotArea', 'TotalBsmtSF'] + [target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train[cols_to_keep].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = train_df.drop(columns=target), train_df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create NN\n",
    "\n",
    "### Initialize Weight and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_units, n_features, n_units_output=1):    \n",
    "    \"\"\"Initialize parameters Weight and bias and stored them\n",
    "    in a dictionnary following this format : {'Wl': tf.Variable, 'bl': tf.Variable, ...}.\n",
    "    \n",
    "    Where `l` is the layer number. \n",
    "    \n",
    "    The dictionnary also store number of layers L (output layer include)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_units: list\n",
    "        list of number of units per layer\n",
    "    n_features: int\n",
    "        number of features at input layer\n",
    "    n_units_output: int (default 1)\n",
    "        number of units at output layer\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict:\n",
    "        Dictionnary of parameters    \n",
    "    \"\"\"\n",
    "    tf.set_random_seed(1) \n",
    "    parameters = {}\n",
    "\n",
    "    n_units += [n_units_output]    \n",
    "    n_layers = len(n_units)\n",
    "    \n",
    "    parameters['L'] = n_layers\n",
    "    \n",
    "    for l in range(0,n_layers):\n",
    "        num_units = n_units[l]   \n",
    "        W = f'W{l}'\n",
    "        b = f'b{l}'\n",
    "\n",
    "        prev_n_units = n_features if l == 0 else n_units[l-1]\n",
    "\n",
    "        # Using Xavier initializer for Weight \n",
    "        # Todo custom\n",
    "        parameters[W] = tf.get_variable(name=W, shape=[prev_n_units, num_units], \n",
    "                                        initializer = tf.contrib.layers.xavier_initializer())\n",
    "        # Using zeros initializer for bias\n",
    "        parameters[b] = tf.get_variable(name=b, shape=[1,num_units], \n",
    "                                        initializer = tf.zeros_initializer())\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'L': 3,\n",
       " 'W0': <tf.Variable 'W0:0' shape=(20, 10) dtype=float32_ref>,\n",
       " 'b0': <tf.Variable 'b0:0' shape=(1, 10) dtype=float32_ref>,\n",
       " 'W1': <tf.Variable 'W1:0' shape=(10, 5) dtype=float32_ref>,\n",
       " 'b1': <tf.Variable 'b1:0' shape=(1, 5) dtype=float32_ref>,\n",
       " 'W2': <tf.Variable 'W2:0' shape=(5, 2) dtype=float32_ref>,\n",
       " 'b2': <tf.Variable 'b2:0' shape=(1, 2) dtype=float32_ref>}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test for a NN with 2 hidden layers, 20 features and 2 output units\n",
    "tf.reset_default_graph()   \n",
    "\n",
    "initialize_parameters(n_units=[10,5], n_features=20, n_units_output=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_layer(A, parameters, layer, activation='relu'):\n",
    "    \"\"\"Compute calculus for a given layer\n",
    "    \n",
    "    Follow this formula : Z_l = A_{l-1} * W_l + b_l\n",
    "    \n",
    "    if activation is set then this function returns : \n",
    "    A_l = activation_function(Z_l) \n",
    "    \n",
    "    Where :\n",
    "    - A is (1, nb units prev layer)\n",
    "    - W is (nb units prev layer, nb units current layer)\n",
    "    - b is (1, nb units current layer)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    A: tf.Variable\n",
    "        output of the previous layer\n",
    "    parameters: dict\n",
    "        Dictionnary of parameters  \n",
    "    layer: int\n",
    "        number of the current layer\n",
    "    activation: str (default relu)\n",
    "        activation function, if None then just \n",
    "        compute the linear calculus\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tf.Variable\n",
    "        Output of the layer calculus\n",
    "    \"\"\"\n",
    "    W = parameters['W'+str(layer)]\n",
    "    b = parameters['b'+str(layer)]\n",
    "    \n",
    "    Z = tf.add(tf.matmul(A,W), b, name=f'Z{layer}')\n",
    "    \n",
    "    if activation is not None:\n",
    "        return tf.nn.relu(Z, name=f'A{layer}') \n",
    "    \n",
    "    return Z\n",
    "\n",
    "def forward_propagation(X, parameters, activation='relu'):\n",
    "    \"\"\"Compute forward propagation given input X and\n",
    "    initialized parameters.\n",
    "    \n",
    "    You can custom activation functionfor the hidden layers\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: tf.Variable\n",
    "        input data shape (n_rows, n_features) \n",
    "    parameters: dict\n",
    "        Dictionnary of parameters  \n",
    "    activation: str (default relu)\n",
    "        activation function for hidden layers\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tf.Variable\n",
    "        Prediction output shape (n_rows, n_units_output)\n",
    "    \"\"\"\n",
    "    A = X\n",
    "    n_layers = parameters['L']\n",
    "    \n",
    "    for l in range(0, n_layers):\n",
    "        act_l = activation if l != n_layers-1 else None\n",
    "        \n",
    "        A = compute_layer(A, parameters, layer=l, activation=act_l)\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Z2:0' shape=(?, 2) dtype=float32>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test for a NN with 2 hidden layers, 20 features and 2 output units\n",
    "tf.reset_default_graph()   \n",
    "\n",
    "parameters = initialize_parameters(n_units=[10,5], n_features=20, n_units_output=2)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, 20), name='X')\n",
    "\n",
    "forward_propagation(X, parameters, activation='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute cost\n",
    "\n",
    "Handle the following cost function :\n",
    "\n",
    "-----\n",
    "\n",
    "- `MSE`: Mean square Error (or Quadratic Loss or L2 Loss)\n",
    "\n",
    "$MSE = \\frac{1}{n}\\sum_{i=1}^{n}{(y_i - \\hat{y_i})^2}$\n",
    "\n",
    "```python\n",
    "def loss(Y_pred, Y_real):\n",
    "    return tf.reduce_mean(tf.square(Y_pred - Y_real))\n",
    "```\n",
    "\n",
    "-----\n",
    "- `RMSE`: Root Mean square Error \n",
    "\n",
    "$RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}{(y_i - \\hat{y_i})^2}}$\n",
    "\n",
    "```python\n",
    "def loss(Y_pred, Y_real):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(Y_pred - Y_real)))\n",
    "```\n",
    "-----\n",
    "- `RMSE log`: Root Mean square Error (log values)\n",
    "\n",
    "$RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}{(\\log{y_i} - \\log{\\hat{y_i}})^2}}$\n",
    "\n",
    "```python\n",
    "def loss(Y_pred, Y_real):\n",
    "    return tf.reduce_mean(tf.abs(Y_pred - Y_real))\n",
    "```\n",
    "\n",
    "-----\n",
    "- `MAE`: Mean Absolute Error (or L1 Loss)\n",
    "\n",
    "$MAE = \\frac{1}{n} \\sum_{i=1}^{n}{|y_i - \\hat{y_i}|}$\n",
    "\n",
    "```python\n",
    "def loss(Y_pred, Y_real):\n",
    "    return tf.reduce_mean(tf.abs(Y_pred - Y_real))\n",
    "```\n",
    "\n",
    "-----\n",
    "- `Entropy`: Cross Entropy Loss (or Negative Log Likelihood)\n",
    "\n",
    "$\\text{Cross Entropy Loss} = - (y_i\\log{\\hat{y_i}} + (1-y_i)\\log{1-\\hat{y_i}})$\n",
    "\n",
    "```python\n",
    "def loss(Y_pred, Y_real):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels, logits))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(Y_pred, Y_real, loss='RMSE'):\n",
    "    \"\"\"Compute loss given a loss function\n",
    "    \n",
    "    Loss available : \n",
    "    - MSE\n",
    "    - RMSE\n",
    "    - RMSE log\n",
    "    - MAE\n",
    "    - Entropy\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Y_pred: tf.Variable\n",
    "        Predictions from the model\n",
    "    Y_real: tf.Variable\n",
    "        Real output\n",
    "    loss: str (default RMSE)\n",
    "        loss function to compute\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tf.Variable:\n",
    "        Loss of the model\n",
    "    \"\"\"\n",
    "    if loss == 'MSE':\n",
    "        return tf.reduce_mean(tf.square(Y_pred - Y_real), name='loss')\n",
    "    \n",
    "    elif loss == 'RMSE':\n",
    "        return tf.sqrt(tf.reduce_mean(tf.square(Y_pred - Y_real)), name='loss')\n",
    "    \n",
    "    elif loss == 'RMSE log':\n",
    "        return tf.reduce_mean(tf.abs(tf.log(Y_pred) - tf.log(Y_real)), name='loss')\n",
    "        \n",
    "    elif loss == 'MAE':\n",
    "        return tf.reduce_mean(tf.abs(Y_pred - Y_real), name='loss')\n",
    "    \n",
    "    elif loss == 'Entropy':\n",
    "        return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(Y_real, Y_pred), name='loss')\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "tf.reset_default_graph()   \n",
    "\n",
    "parameters = initialize_parameters(n_units=[10], n_features=5)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, 5), name='X')\n",
    "Y = tf.placeholder(tf.float32, shape=(None, 1), name='Y')\n",
    "\n",
    "Y_pred = forward_propagation(X, parameters, activation='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'loss:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss(Y_pred=Y_pred, Y_real=Y, loss='MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'loss_1:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss(Y_pred=Y_pred, Y_real=Y, loss='RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'loss_2:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss(Y_pred=Y_pred, Y_real=Y, loss='RMSE log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'loss_3:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss(Y_pred=Y_pred, Y_real=Y, loss='MAE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'loss_4:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss(Y_pred=Y_pred, Y_real=Y, loss='Entropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer selection\n",
    "\n",
    "This the list of possible optimizer to train the neural network :\n",
    "\n",
    "- GradientDescent: [`tf.train.GradientDescentOptimizer`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/train/GradientDescentOptimizer)\n",
    "- Momentum: [`tf.train.MomentumOptimizer`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/train/MomentumOptimizer)\n",
    "- Adagrad: [`tf.train.AdagradOptimizer`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/train/AdagradOptimizer)\n",
    "- Adadelta: [`tf.train.AdadeltaOptimizer`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/train/AdadeltaOptimizer)\n",
    "- RMSProp: [`tf.train.RMSPropOptimizer`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/train/RMSPropOptimizer)\n",
    "- Adam: [`tf.train.AdamOptimizer`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/train/AdamOptimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_optimizer(optimizer='GradientDescent'):\n",
    "    \"\"\"Select the optimizer\n",
    "    \n",
    "    Available optimizers:\n",
    "    - GradientDescent\n",
    "    - Momentum\n",
    "    - Adagrad\n",
    "    - Adadelta\n",
    "    - RMSProp\n",
    "    - Adam\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    optimizer: str (default GradientDescent)\n",
    "        optimizer to select\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tf.train.Optimizer\n",
    "        Optimizer object\n",
    "    \"\"\"\n",
    "    if optimizer == 'GradientDescent':\n",
    "        return tf.train.GradientDescentOptimizer\n",
    "    elif optimizer == 'Momentum':\n",
    "        return tf.train.MomentumOptimizer\n",
    "    elif optimizer == 'Adagrad':\n",
    "        return tf.train.AdagradOptimizer\n",
    "    elif optimizer == 'Adadelta':\n",
    "        return tf.train.AdadeltaOptimizer\n",
    "    elif optimizer == 'RMSProp':\n",
    "        return tf.train.RMSPropOptimizer\n",
    "    elif optimizer == 'Adam':\n",
    "        return tf.train.AdamOptimizer\n",
    "    \n",
    "    raise ValueError('Optimizer choosen not available please select a valid one')\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.training.adam.AdamOptimizer"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_optimizer(optimizer='Adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensorboard(sess, L, grads_and_vars, loss, name='dashboard'):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if not os.path.exists('summaries'):\n",
    "        os.mkdir('summaries')\n",
    "    if not os.path.exists(os.path.join('summaries',name)):\n",
    "        os.mkdir(os.path.join('summaries',name))\n",
    "\n",
    "    summ_writer = tf.summary.FileWriter(os.path.join('summaries',name), sess.graph)    \n",
    "    \n",
    "    # tf_loss_summary :     you feed in a value by means of a placeholder, \n",
    "    #                       whenever you need to publish this to the board\n",
    "    with tf.name_scope('performance'):\n",
    "        tf_loss_ph = tf.placeholder(tf.float32,shape=None,name='loss_summary')\n",
    "        tf_loss_summary = tf.summary.scalar('loss', tf_loss_ph)\n",
    "\n",
    "        \n",
    "    # tf_gradnorm_summary : this calculates the l2 norm of the gradients \n",
    "    #                       of the last layer of your neural network. \n",
    "    #                       Gradient norm is a good indicator of whether the weights \n",
    "    #                       of the neural network are being properly updated. \n",
    "    #                       A too small gradient norm can indicate vanishing gradient \n",
    "    #                       or a too large gradient can imply exploding gradient phenomenon.\n",
    "    for g, v in grads_and_vars:\n",
    "        if f'W{L-1}' in v.name:\n",
    "            with tf.name_scope('gradients'):\n",
    "                tf_last_grad_norm = tf.sqrt(tf.reduce_mean(g**2))\n",
    "                tf_gradnorm_summary = tf.summary.scalar('grad_norm', tf_last_grad_norm)\n",
    "                break\n",
    "                \n",
    "    performance_summaries = tf.summary.merge([tf_loss_summary])    \n",
    "    \n",
    "    return {\n",
    "        'writer': summ_writer, \n",
    "        'loss': tf_loss_ph, \n",
    "        'gradnorm': tf_gradnorm_summary,\n",
    "        'perf': performance_summaries\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate NN with optimizer for back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cost(train_costs, test_costs=None):\n",
    "    \"\"\"Plot the cost every 5 epochs\n",
    "    \"\"\"\n",
    "    plt.plot(np.squeeze(train_costs), label='train')\n",
    "    if test_costs is not None:\n",
    "        plt.plot(np.squeeze(test_costs), label='test')\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per fives)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def fit(feed_dict_train, parameters, optimizer, grads_and_vars, loss, num_epochs,\n",
    "        epsilon, feed_dict_test, batch_size=None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # TODO : implement batch size\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    train_costs = list()\n",
    "    test_costs = list()\n",
    "    num_epochs = 100000000 if num_epochs is None else num_epochs\n",
    "    L = parameters['L']\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        \n",
    "        board = create_tensorboard(sess, L, grads_and_vars, loss, name='dashboard')\n",
    "        summ_writer = board['writer']\n",
    "        tf_loss_ph = board['loss']\n",
    "        tf_gradnorm_summary = board['gradnorm']\n",
    "        performance_summaries = board['perf']\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            sess.run(optimizer, feed_dict=feed_dict_train)\n",
    "            \n",
    "            epoch_l_train, gn_summ = sess.run([loss, tf_gradnorm_summary], feed_dict=feed_dict_train)\n",
    "            epoch_l_test = sess.run(loss, feed_dict=feed_dict_test)\n",
    "            \n",
    "            summ = sess.run(performance_summaries, feed_dict={tf_loss_ph: epoch_l_train})\n",
    "            \n",
    "            summ_writer.add_summary(gn_summ, epoch)\n",
    "            summ_writer.add_summary(summ, epoch)\n",
    "\n",
    "            if epoch % display_freq == 0:\n",
    "                print (\"At epoch %i, train cost is: %f and test cost is: %f\" % (\n",
    "                    epoch, epoch_l_train, epoch_l_test))\n",
    "            if epoch % 5 == 0:\n",
    "                train_costs.append(epoch_l_train)\n",
    "                test_costs.append(epoch_l_test)\n",
    "                \n",
    "                # Early stopping\n",
    "                if len(train_costs) > 2:\n",
    "                    if train_costs[-2] - train_costs[-1] < epsilon:\n",
    "                        print('Stop at epoch %i.'% (epoch))\n",
    "                        break\n",
    "                \n",
    "        # Plot the cost\n",
    "        plot_cost(train_costs, test_costs)\n",
    "                \n",
    "        # lets save the parameters in a variable\n",
    "        del parameters['L']\n",
    "        parameters = sess.run(parameters)\n",
    "        print(\"Parameters have been trained!\")\n",
    "    \n",
    "    sess.close()\n",
    "    return parameters\n",
    "\n",
    "def neural_network(X_train, Y_train, X_test=None, Y_test=None,\n",
    "                   learning_rate=0.01, num_epochs=None, epsilon=0.0001, \n",
    "                   n_units=[1], activation='relu', loss_fn='RMSE', \n",
    "                   optimizer_name='Adam', display_freq=100):\n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: 2 dimensions array like \n",
    "        array shape (n_rows, n_features)\n",
    "    Y_train: array like\n",
    "        array shape (n_rows, 1)\n",
    "    X_test: 2 dimensions array like (optional)\n",
    "        array shape (n_rows, n_features)\n",
    "    Y_test: array like (optional)\n",
    "        array shape (n_rows, 1)\n",
    "    learning_rate: float\n",
    "        Learning rate for the model\n",
    "    num_epochs: int (optional)\n",
    "        number of epoch to train\n",
    "        If None then it stop if the cost difference\n",
    "        is less than epsilon\n",
    "    epsilon: float (default 0.0001)\n",
    "        threshold to stop learning\n",
    "    n_units: list (default [1])\n",
    "        list of units per layer\n",
    "    activation: str (default 'relu')\n",
    "        activation function for hidden layers\n",
    "    loss_fn: str (default RMSE)\n",
    "        loss function to compute\n",
    "    optimizer_name: str (default Adam)\n",
    "        optimizer to use\n",
    "    display_freq: int (default 100)\n",
    "        display frequency to follow the model learning\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict:\n",
    "        Dictionnary of the trained parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    if type(X_train) == pd.DataFrame:\n",
    "        X_train = X_train.values\n",
    "        Y_train = Y_train.values.reshape(-1,1)\n",
    "        \n",
    "    if X_test is not None:\n",
    "        if type(X_test) == pd.DataFrame:\n",
    "            X_test = X_test.values\n",
    "            Y_test = Y_test.values.reshape(-1,1)\n",
    "    \n",
    "    # Reset graph\n",
    "    tf.reset_default_graph()\n",
    "    # Set seed\n",
    "    tf.set_random_seed(42)\n",
    "    \n",
    "    _, n_features = X_train.shape\n",
    "    \n",
    "    # Initialize placeholder    \n",
    "    X = tf.placeholder(tf.float32, shape=(None, n_features), name='X')\n",
    "    Y = tf.placeholder(tf.float32, shape=(None, 1), name='Y')\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_units, n_features, n_units_output=1)\n",
    "    \n",
    "    # Create forward propagation graphs\n",
    "    Y_pred = forward_propagation(X, parameters, activation)\n",
    "    \n",
    "    # Create loss computation graph\n",
    "    loss = compute_loss(Y_pred, Y, loss=loss_fn)\n",
    "    \n",
    "    # Create Optimizer \n",
    "    optimizer_obj = select_optimizer(optimizer_name)(learning_rate)\n",
    "    grads_and_vars = optimizer_obj.compute_gradients(loss)    \n",
    "    optimizer = optimizer_obj.minimize(loss)\n",
    "    \n",
    "    feed_dict_train = {X: X_train, Y: Y_train}\n",
    "    feed_dict_test = {X: X_test, Y: Y_test} if X_test is not None else None\n",
    "    \n",
    "    parameters = fit(feed_dict_train, parameters, optimizer, grads_and_vars, loss, \n",
    "                     num_epochs, epsilon, feed_dict_test)\n",
    "    \n",
    "    return parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 0, train cost is: 4.325962 and test cost is: 4.346067\n",
      "At epoch 100, train cost is: 2.435147 and test cost is: 2.456405\n",
      "At epoch 200, train cost is: 1.530335 and test cost is: 1.545562\n",
      "At epoch 300, train cost is: 0.883988 and test cost is: 0.908920\n",
      "At epoch 400, train cost is: 0.467912 and test cost is: 0.513343\n",
      "At epoch 500, train cost is: 0.356832 and test cost is: 0.402809\n",
      "At epoch 600, train cost is: 0.346679 and test cost is: 0.388927\n",
      "Stop at epoch 630.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters have been trained!\n"
     ]
    }
   ],
   "source": [
    "# Choose hyper parameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = None \n",
    "epsilon = 0.0001\n",
    "n_units = [10,10,10]\n",
    "activation = 'relu' \n",
    "loss_fn = 'RMSE log'\n",
    "optimizer = 'Adam'\n",
    "# batch_size to implement\n",
    "\n",
    "# Helper parameters\n",
    "display_freq = 100\n",
    "\n",
    "parameters = neural_network(X_train, y_train, X_test, y_test,\n",
    "                           learning_rate, num_epochs, epsilon, \n",
    "                           n_units, activation, loss_fn, optimizer,\n",
    "                           display_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard --logdir=\"./summaries/dashboard/\" --port 6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-use trained paramaters to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def predict(X, parameters):\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    X_tf = tf.placeholder(tf.float32, shape=(None, X.shape[1]), name='X')\n",
    "    Y_pred = forward_propagation(X_tf, parameters)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        \n",
    "        preds = sess.run(Y_pred, feed_dict={X_tf: X})\n",
    "                   \n",
    "    sess.close()\n",
    "    return preds\n",
    "\n",
    "\n",
    "def test_model(X_train, Y_train, X_test, Y_test, parameters):\n",
    "    if type(X_train) == pd.DataFrame:\n",
    "        X_train = X_train.values\n",
    "        Y_train = Y_train.values.reshape(-1,1)\n",
    "        X_test = X_test.values\n",
    "        Y_test = Y_test.values.reshape(-1,1)    \n",
    "\n",
    "    pred_train = predict(X_train, parameters)\n",
    "    pred_test = predict(X_test, parameters)\n",
    "    \n",
    "    train_cost = np.sqrt(mean_squared_log_error(Y_train, pred_train ))\n",
    "    test_cost = np.sqrt(mean_squared_log_error(Y_test, pred_test ))\n",
    "\n",
    "    print (\"Train score:\", train_cost)\n",
    "    print (\"Test score:\", test_cost)\n",
    "    \n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
